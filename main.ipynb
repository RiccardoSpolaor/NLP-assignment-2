{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Train, Validation and Test splits\n",
    "\n",
    "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
    "\n",
    "We'll consider the provided validation set as a test set. <br>\n",
    "$\\rightarrow$ Write your own script to:\n",
    "* Split the train data in train and validation splits (80% train and 20% val)\n",
    "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
    "* Perform splitting using the following seed for reproducibility: 42\n",
    "\n",
    "#### Reproducibility Memo\n",
    "\n",
    "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('coqa', 'train.json'), 'r') as j:\n",
    "    train = json.loads(j.read())\n",
    "\n",
    "with open(os.path.join('coqa', 'test.json'), 'r') as j:\n",
    "    test = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train['data']\n",
    "test = test['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[len(doc['questions']) for doc in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=np.cumsum(np.array(lengths,dtype=np.float32))\n",
    "train_end=np.where((le/le[-1])>0.8)[0][0]\n",
    "\n",
    "validation = train[train_end : ] \n",
    "train = train[ : train_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5771\n",
      "1428\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86909 0.7999208445700295\n",
      "21738 0.20007915542997046\n"
     ]
    }
   ],
   "source": [
    "len_train=np.sum([len(doc['questions']) for doc in train])\n",
    "len_val=np.sum([len(doc['questions']) for doc in validation])\n",
    "\n",
    "len_tot=len_train+len_val\n",
    "print(len_train,len_train/len_tot)\n",
    "print(len_val,len_val/len_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, return_history=False):\n",
    "\n",
    "        self.story=[d['story'] for d in data]\n",
    "        self.questions=[d['questions'] for d in data]\n",
    "        self.answers=[d['answers'] for d in data]\n",
    "        lengths = [len(doc['questions']) for doc in data]\n",
    "        self.lengths = np.cumsum(np.array(lengths,dtype=np.int32))\n",
    "        self.R_H=return_history\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lengths[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx=int(np.where(self.lengths > idx)[0][0])\n",
    "        if f_idx>0:\n",
    "            q_idx=idx-self.lengths[f_idx-1]\n",
    "        else:\n",
    "            q_idx=idx\n",
    "\n",
    "        passage=self.story[f_idx]\n",
    "        questions=self.questions[f_idx]\n",
    "        answers=self.answers[f_idx]\n",
    "        question=questions[q_idx]['input_text']\n",
    "        answer=answers[q_idx]['input_text']\n",
    "\n",
    "        if self.R_H:\n",
    "            print([ [questions[i]['input_text'],answers[i]['input_text']] for i in range(q_idx)])\n",
    "            history=np.concatenate([ [questions[i]['input_text'],answers[i]['input_text']] for i in range(q_idx)],0)\n",
    "            return (passage,question,history), answer\n",
    "\n",
    "        return (passage,question), answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(CustomImageDataset(train), batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(CustomImageDataset(validation), batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(CustomImageDataset(test), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a21de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Task 3] Model definition\n",
    "\n",
    "Write your own script to define the following transformer-based models from [huggingface](https://HuggingFace.co/).\n",
    "\n",
    "* [M1] DistilRoBERTa (distilberta-base)\n",
    "* [M2] BERTTiny (bert-tiny)\n",
    "\n",
    "**Note**: Remember to install the ```transformers``` python package!\n",
    "\n",
    "**Note**: We consider small transformer models for computational reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "model_name = 'distilroberta-base'\n",
    "\n",
    "M1 = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name,max_new_tokens=50)\n",
    "T1 = AutoTokenizer.from_pretrained(model_name,max_new_tokens=50)\n",
    "\n",
    "\n",
    "model_name = 'prajjwal1/bert-tiny'\n",
    "\n",
    "M2 = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)\n",
    "T2 = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jefferson's metaphor of a wall of separation has been cited repeatedly by the U.S. Supreme Court. In Reynolds v. United States (1879) the Court wrote that Jefferson's comments \"may be accepted almost as an authoritative declaration of the scope and effect of the [First] Amendment.\" In Everson v. Board of Education (1947), Justice Hugo Black wrote: \"In the words of Thomas Jefferson, the clause against establishment of religion by law was intended to erect a wall of separation between church and state.\" \n",
      "\n",
      "Many early immigrant groups traveled to America to worship freely, particularly after the English Civil War and religious conflict in France and Germany. They included nonconformists like the Puritans, who were Protestant Christians fleeing religious persecution from the Anglican King of England. Despite a common background, the groups' views on religious toleration were mixed. While some such as Roger Williams of Rhode Island and William Penn of Pennsylvania ensured the protection of religious minorities within their colonies, others like the Plymouth Colony and Massachusetts Bay Colony had established churches. The Dutch colony of New Netherland established the Dutch Reformed Church and outlawed all other worship, though enforcement was sparse. Religious conformity was desired partly for financial reasons: the established Church was responsible for poverty relief, putting dissenting churches at a significant disadvantage. [SEP] from?\n"
     ]
    }
   ],
   "source": [
    "i=42\n",
    "\n",
    "passage=train[i]['story']\n",
    "questions=train[i]['questions']\n",
    "n=random.randint(0,len(questions))\n",
    "question=questions[n]['input_text']\n",
    "\n",
    "input_text=passage+' [SEP] '+question\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['after what did a a lot of migrants travel?', 'English Civil War',\n",
       "       'where did they go?', 'traveled to America', 'who were they?',\n",
       "       'nonconformists like the Puritans', 'why did they leave home?',\n",
       "       'fleeing religious persecution', 'who persecuted them?',\n",
       "       'Anglican King of England.',\n",
       "       'did they all share the same viewpoint on theology?', 'No',\n",
       "       'did some protect different ideas?', 'yes', 'who was one?',\n",
       "       'Roger Williams', 'from where?', 'Rhode Island', 'and another?',\n",
       "       'William Penn'], dtype='<U50')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history=np.concatenate([ [train[i]['questions'][idx]['input_text'],train[i]['answers'][idx]['input_text']] for idx in range(n)],0)\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jefferson\\'s metaphor of a wall of separation has been cited repeatedly by the U.S. Supreme Court. In Reynolds v. United States (1879) the Court wrote that Jefferson\\'s comments \"may be accepted almost as an authoritative declaration of the scope and effect of the [First] Amendment.\" In Everson v. Board of Education (1947), Justice Hugo Black wrote: \"In the words of Thomas Jefferson, the clause against establishment of religion by law was intended to erect a wall of separation between church and state.\" \\n\\nMany early immigrant groups traveled to America to worship freely, particularly after the English Civil War and religious conflict in France and Germany. They included nonconformists like the Puritans, who were Protestant Christians fleeing religious persecution from the Anglican King of England. Despite a common background, the groups\\' views on religious toleration were mixed. While some such as Roger Williams of Rhode Island and William Penn of Pennsylvania ensured the protection of religious minorities within their colonies, others like the Plymouth Colony and Massachusetts Bay Colony had established churches. The Dutch colony of New Netherland established the Dutch Reformed Church and outlawed all other worship, though enforcement was sparse. Religious conformity was desired partly for financial reasons: the established Church was responsible for poverty relief, putting dissenting churches at a significant disadvantage. [SEP] after what did a a lot of migrants travel? [SEP] English Civil War [SEP] where did they go? [SEP] traveled to America [SEP] who were they? [SEP] nonconformists like the Puritans [SEP] why did they leave home? [SEP] fleeing religious persecution [SEP] who persecuted them? [SEP] Anglican King of England. [SEP] did they all share the same viewpoint on theology? [SEP] No [SEP] did some protect different ideas? [SEP] yes [SEP] who was one? [SEP] Roger Williams [SEP] from where? [SEP] Rhode Island [SEP] and another? [SEP] William Penn [SEP] from?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separator = ' [SEP] '\n",
    "text_input = passage + f'{separator if len(history) else \"\"}' + separator.join(history) + separator + question\n",
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_PQ(model, tokenizer, passage, question):\n",
    "    text_input = question + ' [SEP] ' + passage\n",
    "    input_ids = tokenizer(text_input, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = model.generate(input_ids)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "c:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "',,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_PQ(M1, T1, ' ', question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_PQH(model, tokenizer, passage, question, history):\n",
    "    separator = ' [SEP] '\n",
    "    text_input = question + f'{separator if len(history) else \"\"}' + separator.join(history)+ separator+ passage \n",
    "    input_ids = tokenizer(text_input, return_tensors=\"pt\").input_ids\n",
    "    generated_ids = model.generate(input_ids)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer):\n",
    "    model.to('cuda')\n",
    "\n",
    "    L=[]\n",
    "\n",
    "    model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (passage, question), answer = data\n",
    "\n",
    "            text_input = [question[i] + ' [SEP] ' + passage[i] for i in range(len(passage))]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = tokenizer(\n",
    "                text_input,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "            labels = tokenizer(\n",
    "                answer,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "            X=torch.tensor(input_ids,device='cuda')\n",
    "            y=torch.tensor(labels,device='cuda')\n",
    "            print(X.shape,y.shape)\n",
    "            if X.shape[1]>500:\n",
    "                continue\n",
    "\n",
    "            # the forward function automatically creates the correct decoder_input_ids\n",
    "            loss = model(input_ids=X, labels=y).loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            L.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3g}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_8568\\4025953688.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X=torch.tensor(input_ids,device='cuda')\n",
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_8568\\4025953688.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(labels,device='cuda')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 443]) torch.Size([1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.00458\n",
      "torch.Size([1, 369]) torch.Size([1, 4])\n",
      "[1,     2] loss: 0.00259\n",
      "torch.Size([1, 447]) torch.Size([1, 3])\n",
      "[1,     3] loss: 0.0026\n",
      "torch.Size([1, 260]) torch.Size([1, 11])\n",
      "[1,     4] loss: 0.0117\n",
      "torch.Size([1, 345]) torch.Size([1, 4])\n",
      "[1,     5] loss: 0.00639\n",
      "torch.Size([1, 373]) torch.Size([1, 5])\n",
      "[1,     6] loss: 0.00458\n",
      "torch.Size([1, 357]) torch.Size([1, 3])\n",
      "[1,     7] loss: 0.0027\n",
      "torch.Size([1, 446]) torch.Size([1, 8])\n",
      "[1,     8] loss: 0.00425\n",
      "torch.Size([1, 324]) torch.Size([1, 5])\n",
      "[1,     9] loss: 0.00285\n",
      "torch.Size([1, 383]) torch.Size([1, 5])\n",
      "[1,    10] loss: 0.00246\n",
      "torch.Size([1, 429]) torch.Size([1, 4])\n",
      "[1,    11] loss: 0.00336\n",
      "torch.Size([1, 371]) torch.Size([1, 6])\n",
      "[1,    12] loss: 0.00351\n",
      "torch.Size([1, 378]) torch.Size([1, 3])\n",
      "[1,    13] loss: 0.00197\n",
      "torch.Size([1, 355]) torch.Size([1, 9])\n",
      "[1,    14] loss: 0.0037\n",
      "torch.Size([1, 398]) torch.Size([1, 3])\n",
      "[1,    15] loss: 0.0018\n",
      "torch.Size([1, 382]) torch.Size([1, 6])\n",
      "[1,    16] loss: 0.00377\n",
      "torch.Size([1, 378]) torch.Size([1, 3])\n",
      "[1,    17] loss: 0.00224\n",
      "torch.Size([1, 372]) torch.Size([1, 8])\n",
      "[1,    18] loss: 0.00372\n",
      "torch.Size([1, 308]) torch.Size([1, 3])\n",
      "[1,    19] loss: 0.00232\n",
      "torch.Size([1, 283]) torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.0029\n",
      "torch.Size([1, 366]) torch.Size([1, 3])\n",
      "[1,    21] loss: 0.00226\n",
      "torch.Size([1, 552]) torch.Size([1, 3])\n",
      "torch.Size([1, 413]) torch.Size([1, 3])\n",
      "[1,    23] loss: 0.0015\n",
      "torch.Size([1, 406]) torch.Size([1, 3])\n",
      "[1,    24] loss: 0.00238\n",
      "torch.Size([1, 424]) torch.Size([1, 3])\n",
      "[1,    25] loss: 0.00216\n",
      "torch.Size([1, 329]) torch.Size([1, 9])\n",
      "[1,    26] loss: 0.00437\n",
      "torch.Size([1, 424]) torch.Size([1, 3])\n",
      "[1,    27] loss: 0.00231\n",
      "torch.Size([1, 364]) torch.Size([1, 3])\n",
      "[1,    28] loss: 0.00261\n",
      "torch.Size([1, 396]) torch.Size([1, 7])\n",
      "[1,    29] loss: 0.0036\n",
      "torch.Size([1, 251]) torch.Size([1, 4])\n",
      "[1,    30] loss: 0.00232\n",
      "torch.Size([1, 371]) torch.Size([1, 3])\n",
      "[1,    31] loss: 0.00166\n",
      "torch.Size([1, 453]) torch.Size([1, 4])\n",
      "[1,    32] loss: 0.00324\n",
      "torch.Size([1, 427]) torch.Size([1, 4])\n",
      "[1,    33] loss: 0.00336\n",
      "torch.Size([1, 299]) torch.Size([1, 3])\n",
      "[1,    34] loss: 0.00239\n",
      "torch.Size([1, 355]) torch.Size([1, 7])\n",
      "[1,    35] loss: 0.00402\n",
      "torch.Size([1, 307]) torch.Size([1, 5])\n",
      "[1,    36] loss: 0.00326\n",
      "torch.Size([1, 335]) torch.Size([1, 8])\n",
      "[1,    37] loss: 0.00371\n",
      "torch.Size([1, 340]) torch.Size([1, 3])\n",
      "[1,    38] loss: 0.00139\n",
      "torch.Size([1, 405]) torch.Size([1, 3])\n",
      "[1,    39] loss: 0.00113\n",
      "torch.Size([1, 343]) torch.Size([1, 3])\n",
      "[1,    40] loss: 0.00198\n",
      "torch.Size([1, 322]) torch.Size([1, 3])\n",
      "[1,    41] loss: 0.00117\n",
      "torch.Size([1, 378]) torch.Size([1, 8])\n",
      "[1,    42] loss: 0.00433\n",
      "torch.Size([1, 295]) torch.Size([1, 4])\n",
      "[1,    43] loss: 0.00328\n",
      "torch.Size([1, 285]) torch.Size([1, 5])\n",
      "[1,    44] loss: 0.0034\n",
      "torch.Size([1, 428]) torch.Size([1, 5])\n",
      "[1,    45] loss: 0.00294\n",
      "torch.Size([1, 352]) torch.Size([1, 3])\n",
      "[1,    46] loss: 0.000986\n",
      "torch.Size([1, 819]) torch.Size([1, 8])\n",
      "torch.Size([1, 386]) torch.Size([1, 4])\n",
      "[1,    48] loss: 0.00287\n",
      "torch.Size([1, 400]) torch.Size([1, 3])\n",
      "[1,    49] loss: 0.0013\n",
      "torch.Size([1, 468]) torch.Size([1, 7])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.18 GiB already allocated; 0 bytes free; 3.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Sam\\Scuola\\02_UniBo\\2_anno\\NLP\\Projects\\NLP-assignment-2\\main.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Sam/Scuola/02_UniBo/2_anno/NLP/Projects/NLP-assignment-2/main.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(M1,T1)\n",
      "\u001b[1;32mc:\\Sam\\Scuola\\02_UniBo\\2_anno\\NLP\\Projects\\NLP-assignment-2\\main.ipynb Cell 28\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sam/Scuola/02_UniBo/2_anno/NLP/Projects/NLP-assignment-2/main.ipynb#X63sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# the forward function automatically creates the correct decoder_input_ids\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sam/Scuola/02_UniBo/2_anno/NLP/Projects/NLP-assignment-2/main.ipynb#X63sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39mX, labels\u001b[39m=\u001b[39my)\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Sam/Scuola/02_UniBo/2_anno/NLP/Projects/NLP-assignment-2/main.ipynb#X63sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sam/Scuola/02_UniBo/2_anno/NLP/Projects/NLP-assignment-2/main.ipynb#X63sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Sam/Scuola/02_UniBo/2_anno/NLP/Projects/NLP-assignment-2/main.ipynb#X63sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.18 GiB already allocated; 0 bytes free; 3.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(M1,T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "506daaeaa79bc5d30715519e2bba71fd2fb898b1f12d903345e89400e3b4f753"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
