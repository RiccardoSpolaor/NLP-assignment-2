import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
from typing import List, Tuple

from models.model import Model

from utils.dataloader_builder import get_dataloader
from utils.squad import compute_squad_f1


def get_worst_answers(model: Model, df_source: pd.DataFrame, use_history: bool, n_worst_answers: int = 5, 
                      evaluation_batch_size: int = 16) -> List[Tuple[float, str, str, str, str, str]]:
    """Get the worst answers generated by the model for a certain source along with their statistics.

    Parameters
    ----------
    model : Model
        The model that is used to generate the answers.
    df_source : DataFrame
        The dataframe containing all the discussions relative to a specific source.
    use_history : bool 
        Whether to use the discussion history to generate the answers or not.
    n_worst_answers : int, optional
        The number of worst generated answers to show. Defaults to 5.
    evaluation_batch_size : int, optional
        The size of the batch used during the generation of the answers. Defaults to 16.

    Returns
    -------
    list of (float, str, str, str, str, str):
        The list of worst generated answers, each represented by a tuple containing:
        * The SQuAD F1 score;
        * The question relative to the generated answer;
        * The passage relative to the question of the generated answer;
        * The previous discussion history relative to the generated answer;
        * The true answer;
        * The predicted answer;
        * The index of the character from which the span of the passage relative to the question starts;
        * The index of the character from which the span of the passage relative to the question ends.
    """
    # initialize worst answers list.
    # Element structure: (SQuAD F1 score, question, passage, history, gold answer, predicted answer, span start, span end)
    worst_answers = []

    torch.cuda.empty_cache()

    # Get dataloader from the source dataframe
    source_dataloader = get_dataloader(df=df_source, batch_size=evaluation_batch_size)

    for data in tqdm(source_dataloader):
        with torch.no_grad():
            # Get batch data
            (passages, questions, histories), (answers, span_starts, span_ends) = data

            # Generate the predicted answers
            predictions = model.generate(passages, questions, histories if use_history else None)

            # Compute the SQuAD F1 scores of each generated answer
            squad_f1_scores = np.array([compute_squad_f1(gold, predicted) for gold, predicted in zip(answers, predictions)])

            # Get the indices of the `n_worst_answers` worst generated answers according to their SQuAD F1 scores
            samples_indices = np.argsort(squad_f1_scores)[:n_worst_answers]

            # Add to the worst answers list (`worst_answers`) the `n_worst_answers` worst generated answers according to their indices
            worst_answers += [
                tuple([
                    squad_f1_scores[sample_idx],
                    questions[sample_idx],
                    passages[sample_idx],
                    histories[sample_idx], 
                    answers[sample_idx],
                    predictions[sample_idx],
                    span_starts[sample_idx],
                    span_ends[sample_idx]
                    ]) for sample_idx in samples_indices
                ]
            # Reduce the `worst_answers` list to the `n_worst_answers` worst generated answers overall according to their SQuAD F1 scores
            worst_answers = sorted(worst_answers)[:n_worst_answers]

    return worst_answers

def show_worst_errors(source_name: str, sources_statistics_dict: dict, show_history: bool = False) -> None:
    """Show the worst generated answers for a given source

    Parameters
    ----------
    source_name : str
        The source name for which the worst answers are shown.
    sources_statistics_dict : dict
        The dictionary containing:
        * The source names as keys; 
        * The worst generated answers related to the given source name along with their statistics.
    show_history : bool, optional 
        Whether to show the previous discussion history or not. Defaults to False.
    """
    results = sources_statistics_dict[source_name]
    for r in results:
        f1_squad, question, _, history, gold_answer, predicted_answer, _, _ = r
        
        print(f'* Question: "{question}"')
        
        if show_history:
            history = history.split('<sep>')
            if len(history) == 0:
                print('* History: N/A')
            else:
                questions = history[::2]
                answers = history[1::2]
                questions_and_answers = [f'Q{i+1}: "{q}"; A{i+1}: "{a}"' for i, (q, a) in enumerate(zip(questions, answers))]
                questions_and_answers = questions_and_answers[-min(2, len(questions_and_answers)):]
                history_string = '; '.join(questions_and_answers)
                print(f'* History: {history_string}')

        print(f'* Gold Answer: "{gold_answer}"')

        print(f'* Predicted Answer: "{predicted_answer}"')

        print(f'* F1 SQuAD: {f1_squad}')
        print()

def plot_token_importances(source_name: str, sources_statistics_dict: dict, model: Model, use_history: bool) -> None:
    """Plot the token importances results for the worst answers of a specific source.

    Parameters
    ----------
    source_name : str
        The source name for which the worst answers are shown.
    sources_statistics_dict : dict
        The dictionary containing:
        * The source names as keys; 
        * The worst generated answers related to the given source name along with their statistics.
    model : Model
        The model that is used to generate the token importances.
    use_history : bool
        Whether to use the discussion history to generate the answers or not.
    """
    results = sources_statistics_dict[source_name]

    n_results = len(results)
    n_cols = 2

    # Compute number of rows in the plot
    n_rows = n_results // n_cols 

    # Add one extra row if necessary
    if n_results % n_cols != 0:
        n_rows += 1

    # Create a position index
    position_range = np.arange(1, n_results + 1)

    fig = plt.figure(figsize=(15, 10))
    
    fig.suptitle(f'Token importances of the {n_results} worst results.')

    for i in range(n_results):
        _, question, passage, history, _, _, span_start, span_end = results[i]

        # Compute token importances
        token_importances = model.compute_token_importances(passage, question, span_start, span_end,
                                                            history if use_history else None)

        # Create the array of the actual tokens present in the span of the given question
        golden_token_span = np.zeros(shape=(token_importances.shape[1],))
        golden_token_span[span_start : span_end] = 1

        ax = fig.add_subplot(n_rows, n_cols, position_range[i])
        # Plot predicted token importances
        ax.plot(token_importances.cpu().detach()[0], label='Predicted token importances')
        # Plot actual token importances
        ax.plot(golden_token_span, label='Golden token importances')
        ax.set_xlabel('Token ids')
        ax.set_ylabel('Importances')
        ax.set_title(f'Result {i + 1}')
        ax.legend()

    plt.subplots_adjust(hspace=.5)

    plt.show()
