{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import time\n",
        "import json\n"
      ],
      "metadata": {
        "id": "3_osvwdxjgyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g2aaIyRm77p",
        "outputId": "c5a27cb1-e22e-416d-f31d-0a44f8733d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMIT6vqyh__Q",
        "outputId": "9ca3b2e9-30f4-4131-8e76-0eb4a0c5cded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "model_name = 'distilroberta-base'\n",
        "\n",
        "M1 = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "T1 = AutoTokenizer.from_pretrained(model_name, max_new_tokens=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinAttention(nn.Module):\n",
        "    def __init__(self, old_attention_layer, k_dims=128, max_input_len=512):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_attention_heads = old_attention_layer.num_attention_heads\n",
        "        self.attention_head_size = old_attention_layer.attention_head_size\n",
        "        self.all_head_size       = old_attention_layer.all_head_size\n",
        "\n",
        "        self.query = old_attention_layer.query\n",
        "        self.key   = old_attention_layer.key\n",
        "        self.value = old_attention_layer.value\n",
        "\n",
        "        self.dropout = old_attention_layer.dropout\n",
        "        self.position_embedding_type = old_attention_layer.position_embedding_type\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = old_attention_layer.max_position_embeddings\n",
        "            self.distance_embedding      = old_attention_layer.distance_embedding\n",
        "\n",
        "        self.is_decoder = old_attention_layer.is_decoder\n",
        "\n",
        "        E = torch.randn(k_dims, max_input_len)/math.sqrt(k_dims)\n",
        "        D = torch.randn(k_dims, max_input_len)/math.sqrt(k_dims)\n",
        "        self.E = nn.Parameter(E)\n",
        "        self.D = nn.Parameter(D) \n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if torch.any(torch.isnan(hidden_states)):\n",
        "            print('0')\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if torch.any(torch.isnan(key_layer)):\n",
        "            print('1_1')\n",
        "        if torch.any(torch.isnan(value_layer)):\n",
        "            print('1_2')\n",
        "        if torch.any(torch.isnan(query_layer)):\n",
        "            print('1_3')\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        \n",
        "        n_input = key_layer.shape[-2]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.shape[-2]!=1:\n",
        "                raise NotImplementedError(\" Linformer not compatible with attention 2dim attention masks\")\n",
        "            else:\n",
        "                n_input = torch.max(torch.sum(attention_mask==0,dim=(1,2,3)))\n",
        "\n",
        "        #print(torch.mean(key_layer),torch.var(key_layer),'k')\n",
        "        #print(torch.mean(value_layer),torch.var(value_layer),'v')\n",
        "\n",
        "        projected_keys = torch.matmul(self.E[:,:n_input], key_layer[:,:,:n_input])\n",
        "        projected_values = torch.matmul(self.D[:,:n_input], value_layer[:,:,:n_input])\n",
        "\n",
        "        #print(torch.mean(self.E),torch.var(self.E),'E')\n",
        "\n",
        "        #print(torch.mean(projected_keys),torch.var(projected_keys),'pk')\n",
        "        #print(torch.mean(projected_values),torch.var(projected_values),'pv')\n",
        "\n",
        "        if torch.any(torch.isnan(projected_keys)):\n",
        "            print('2_1')\n",
        "        if torch.any(torch.isnan(projected_values)):\n",
        "            print('2_2')\n",
        "\n",
        "        #print(torch.mean(query_layer),torch.var(query_layer),'q')\n",
        "\n",
        "        #attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        projected_attention_scores = torch.matmul(query_layer, projected_keys.transpose(-1, -2))\n",
        "\n",
        "        if torch.any(torch.isnan(projected_attention_scores)):\n",
        "            print('3_1')\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            raise NotImplementedError(\" Linformer not compatible with relative keys\")\n",
        "\n",
        "\n",
        "        #attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        projected_attention_scores = projected_attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        #print(torch.mean(projected_attention_scores),torch.var(projected_attention_scores),'s')\n",
        "\n",
        "        if torch.any(torch.isnan(projected_attention_scores)):\n",
        "            print('3_2')\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        #attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        projected_attention_probs = nn.functional.softmax(projected_attention_scores, dim=-1)\n",
        "\n",
        "        if torch.any(torch.isnan(projected_attention_probs)):\n",
        "            print('4_1')\n",
        "            print(projected_attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        #attention_probs = self.dropout(attention_probs)\n",
        "        projected_attention_probs = self.dropout(projected_attention_probs)\n",
        "\n",
        "        if torch.any(torch.isnan(projected_attention_probs)):\n",
        "            print('4_2')\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            projected_attention_probs = projected_attention_probs * head_mask\n",
        "\n",
        "        if torch.any(torch.isnan(projected_attention_probs)):\n",
        "            print('4_3')\n",
        "\n",
        "        #context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = torch.matmul(projected_attention_probs, projected_values)\n",
        "\n",
        "        #print(torch.mean(context_layer),torch.var(context_layer),'C')\n",
        "\n",
        "        if torch.any(torch.isnan(context_layer)):\n",
        "            print('5_1')\n",
        "\n",
        "        #print(projected_attention_probs)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, projected_attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "y8tKIqZRmNYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3--Usv_jeQ1",
        "outputId": "f63909a7-67d1-47a7-8abe-0cf622d2e713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForQuestionAnswering(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for L in M1.roberta.encoder.layer:\n",
        "    A1=LinAttention(L.attention.self, k_dims=64)\n",
        "    L.attention.self=A1"
      ],
      "metadata": {
        "id": "E8Zvy910o2Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test modified model"
      ],
      "metadata": {
        "id": "Zfn1_f5dq6Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "        \n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")"
      ],
      "metadata": {
        "id": "cH20_lBEpyFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
      ],
      "metadata": {
        "id": "WuLMLnrMqKx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('coqa', 'train.json'), 'r') as j:\n",
        "    train = json.loads(j.read())\n",
        "\n",
        "with open(os.path.join('coqa', 'test.json'), 'r') as j:\n",
        "    test = json.loads(j.read())"
      ],
      "metadata": {
        "id": "eeBU_gKIqL5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train['data']\n",
        "test = test['data']"
      ],
      "metadata": {
        "id": "61cPdU93qMtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lengths=[len(doc['questions']) for doc in train]"
      ],
      "metadata": {
        "id": "1_xc2GDgqNYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le=np.cumsum(np.array(lengths,dtype=np.float32))\n",
        "train_end=np.where((le/le[-1])>0.8)[0][0]\n",
        "\n",
        "validation = train[train_end : ] \n",
        "train = train[ : train_end]"
      ],
      "metadata": {
        "id": "P9pRwS3JqOF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train))\n",
        "print(len(validation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue3pl-4LqPEp",
        "outputId": "cbf6f583-b01f-49f2-b65e-4ced3298af88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5771\n",
            "1428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_train=np.sum([len(doc['questions']) for doc in train])\n",
        "len_val=np.sum([len(doc['questions']) for doc in validation])\n",
        "\n",
        "len_tot=len_train+len_val\n",
        "print(len_train,len_train/len_tot)\n",
        "print(len_val,len_val/len_tot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8uQ8ou7qfPx",
        "outputId": "d08378d5-9dcc-4f96-cd60-04ecce763942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86909 0.7999208445700295\n",
            "21738 0.20007915542997046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, return_history=False):\n",
        "\n",
        "        self.story=[d['story'] for d in data]\n",
        "        self.questions=[d['questions'] for d in data]\n",
        "        self.answers=[d['answers'] for d in data]\n",
        "        lengths = [len(doc['questions']) for doc in data]\n",
        "        self.lengths = np.cumsum(np.array(lengths,dtype=np.int32))\n",
        "        self.R_H=return_history\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.lengths[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_idx=int(np.where(self.lengths > idx)[0][0])\n",
        "        if f_idx>0:\n",
        "            q_idx=idx-self.lengths[f_idx-1]\n",
        "        else:\n",
        "            q_idx=idx\n",
        "\n",
        "        passage=self.story[f_idx]\n",
        "        questions=self.questions[f_idx]\n",
        "        answers=self.answers[f_idx]\n",
        "        question=questions[q_idx]['input_text']\n",
        "        span_start=int(answers[q_idx]['span_start'])\n",
        "        span_end=int(answers[q_idx]['span_end'])\n",
        "        span_text=answers[q_idx]['span_text']\n",
        "\n",
        "        if self.R_H:\n",
        "            history = np.concatenate([ [questions[i]['input_text'], answers[i]['input_text']] for i in range(q_idx)],0)\n",
        "            return (passage,question,history), (span_start,span_end)\n",
        "\n",
        "        return (passage,question), (span_start,span_end)"
      ],
      "metadata": {
        "id": "yfayHgIcqgdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=16\n",
        "\n",
        "train_dataloader = DataLoader(CustomImageDataset(train), batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(CustomImageDataset(validation), batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(CustomImageDataset(test), batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZkEFhTGqqiLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_indices(inputs,sep_starts, sep_ends):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
        "        sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
        "        start_char = sep_starts[sample_idx]\n",
        "        end_char = sep_ends[sample_idx]\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label is (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    return start_positions, end_positions"
      ],
      "metadata": {
        "id": "M4Wq505PqkRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(tokenizer,questions, passages, max_length=512, stride=250):\n",
        "        return tokenizer(\n",
        "            questions,\n",
        "            passages,\n",
        "            max_length=max_length,\n",
        "            truncation=\"only_second\",\n",
        "            stride=50,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\", \n",
        "            return_tensors=\"pt\"\n",
        "        )"
      ],
      "metadata": {
        "id": "E7dze4UzqnXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_f(P0,P1,i,j,l1=1,l2=2):\n",
        "\n",
        "    gamma=2 # focal loss parameter\n",
        "\n",
        "    j=i+5\n",
        "\n",
        "    l=len(P0)\n",
        "\n",
        "    S=torch.exp(P0[:,None]+P1[None])\n",
        "    P=torch.triu(S)/(torch.sum(torch.triu(S))+1e-9)\n",
        "\n",
        "    W0=torch.zeros(l,device=P0.device)\n",
        "    W1=torch.zeros(l,device=P0.device)\n",
        "\n",
        "    if i>1:\n",
        "        W0[:i]=1+l1*torch.arange(i,0,-1)/l\n",
        "    if i<l-2:\n",
        "        W0[i+1:]=1+l2*torch.arange(1,l-i)/l\n",
        "    if j>1:\n",
        "        W1[:j]=1+l2*torch.arange(j,0,-1)/l\n",
        "    if j<l-2:\n",
        "        W1[j+1:]=1+l1*torch.arange(1,l-j)/l\n",
        "    \n",
        "    #                     spatial weighting          focal loss      continuous CE loss\n",
        "    return torch.sum( (-(W0[:,None]+W1[None,:]) * torch.pow(P,gamma) * torch.log(1-P)) ) - torch.pow(1-P[i,j],gamma)*torch.log(P[i,j])"
      ],
      "metadata": {
        "id": "_QmoewqDqpA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer, epochs=1, learning_rate=1e-3):\n",
        "    \n",
        "    model.to('cuda')\n",
        "\n",
        "    loss_history=[]\n",
        "\n",
        "    model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    optimizer = torch.optim.Adam(iter(list(model.parameters())), lr=learning_rate)\n",
        "    \n",
        "    max_length = 500\n",
        "    stride = 250\n",
        "\n",
        "    \n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            (passage, question), (sep_starts, sep_ends) = data\n",
        "\n",
        "            #text_input = [question[i] + ' [SEP] ' + passage[i] for i in range(len(passage))]\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            try:\n",
        "                inputs = encode(tokenizer, question, passage, max_length=max_length, stride=stride)\n",
        "            except:\n",
        "                inputs = encode(tokenizer, question, passage, max_length=max_length*2, stride=stride)\n",
        "\n",
        "            start_positions, end_positions= get_target_indices(inputs, sep_starts, sep_ends)\n",
        "\n",
        "            target_start_index = torch.tensor(start_positions)[inputs['overflow_to_sample_mapping']].to('cuda')\n",
        "            target_end_index = torch.tensor(end_positions)[inputs['overflow_to_sample_mapping']].to('cuda')\n",
        "\n",
        "            del inputs['overflow_to_sample_mapping']\n",
        "            del inputs['offset_mapping']\n",
        "\n",
        "            outputs = model(**inputs.to('cuda'), start_positions=target_start_index, end_positions=target_end_index)\n",
        "\n",
        "            #loss = outputs\n",
        "            loss=0\n",
        "            for i in range(len(target_start_index)):\n",
        "                loss+=loss_f(outputs['start_logits'][i],outputs['end_logits'][i],target_start_index[i],target_end_index[i],0.5,1)\n",
        "                loss/=len(target_start_index)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss\n",
        "\n",
        "            loss_history.append(loss.detach().cpu().numpy())\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            batch_time = epoch_time/(batch_idx+1)\n",
        "\n",
        "            print(f\"epoch: {epoch + 1}/{epochs}, {batch_idx + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(batch_idx+1):.3g}, {loss:.3g}              \")#, end = '\\r'\n",
        "\n",
        "        #print(f\"epoch: {epoch + 1}/{epochs}, {batch_idx + 1}/{len(train_dataloader)}, {epoch_time:.0f}s {batch_time*1e3:.0f}ms/step, lr: {optimizer.param_groups[0]['lr']:.3g}, loss: {running_loss/(batch_idx+1):.3g} {loss:.3g}                \")\n",
        "\n",
        "    return loss_history"
      ],
      "metadata": {
        "id": "Fv7TjQOmqqfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R=[]    # save history of the runs"
      ],
      "metadata": {
        "id": "i2OhdYuLqrud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR=[1e-4]  # lr to test         [1e-2,1e-3,1e-4,1e-5]\n",
        "\n",
        "model_name = 'distilroberta-base'\n",
        "#model_name = 'prajjwal1/bert-tiny'\n",
        "\n",
        "T1 = AutoTokenizer.from_pretrained(model_name, max_new_tokens=50)\n",
        "\n",
        "for lr in LR:\n",
        "    #M1 = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    H=train(M1,T1,epochs=1,learning_rate=lr)\n",
        "    R.append([H,lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bkfGTwuQqtNd",
        "outputId": "13181566-0cce-4904-aa5b-21f69178dd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/1, 1/5432, 1s 780ms/step, lr: 0.0001, loss: 0.781, 0.781              \n",
            "epoch: 1/1, 2/5432, 2s 757ms/step, lr: 0.0001, loss: 0.757, 0.733              \n",
            "epoch: 1/1, 3/5432, 2s 747ms/step, lr: 0.0001, loss: 0.767, 0.788              \n",
            "epoch: 1/1, 4/5432, 3s 772ms/step, lr: 0.0001, loss: 0.748, 0.691              \n",
            "epoch: 1/1, 5/5432, 4s 792ms/step, lr: 0.0001, loss: 0.745, 0.734              \n",
            "epoch: 1/1, 6/5432, 5s 796ms/step, lr: 0.0001, loss: 0.743, 0.73              \n",
            "epoch: 1/1, 7/5432, 6s 808ms/step, lr: 0.0001, loss: 0.748, 0.781              \n",
            "epoch: 1/1, 8/5432, 6s 801ms/step, lr: 0.0001, loss: 0.753, 0.786              \n",
            "epoch: 1/1, 9/5432, 7s 812ms/step, lr: 0.0001, loss: 0.745, 0.684              \n",
            "epoch: 1/1, 10/5432, 8s 821ms/step, lr: 0.0001, loss: 0.744, 0.733              \n",
            "epoch: 1/1, 11/5432, 9s 816ms/step, lr: 0.0001, loss: 0.748, 0.784              \n",
            "epoch: 1/1, 12/5432, 10s 815ms/step, lr: 0.0001, loss: 0.747, 0.734              \n",
            "epoch: 1/1, 13/5432, 11s 811ms/step, lr: 0.0001, loss: 0.749, 0.784              \n",
            "epoch: 1/1, 14/5432, 11s 807ms/step, lr: 0.0001, loss: 0.752, 0.78              \n",
            "epoch: 1/1, 15/5432, 12s 803ms/step, lr: 0.0001, loss: 0.754, 0.788              \n",
            "epoch: 1/1, 16/5432, 13s 806ms/step, lr: 0.0001, loss: 0.753, 0.732              \n",
            "epoch: 1/1, 17/5432, 14s 808ms/step, lr: 0.0001, loss: 0.754, 0.782              \n",
            "epoch: 1/1, 18/5432, 15s 818ms/step, lr: 0.0001, loss: 0.749, 0.651              \n",
            "epoch: 1/1, 19/5432, 16s 820ms/step, lr: 0.0001, loss: 0.748, 0.729              \n",
            "epoch: 1/1, 20/5432, 16s 820ms/step, lr: 0.0001, loss: 0.747, 0.734              \n",
            "epoch: 1/1, 21/5432, 17s 815ms/step, lr: 0.0001, loss: 0.749, 0.795              \n",
            "epoch: 1/1, 22/5432, 18s 813ms/step, lr: 0.0001, loss: 0.748, 0.733              \n",
            "epoch: 1/1, 23/5432, 19s 811ms/step, lr: 0.0001, loss: 0.748, 0.733              \n",
            "epoch: 1/1, 24/5432, 19s 808ms/step, lr: 0.0001, loss: 0.749, 0.778              \n",
            "epoch: 1/1, 25/5432, 20s 805ms/step, lr: 0.0001, loss: 0.751, 0.788              \n",
            "epoch: 1/1, 26/5432, 21s 802ms/step, lr: 0.0001, loss: 0.752, 0.787              \n",
            "epoch: 1/1, 27/5432, 22s 803ms/step, lr: 0.0001, loss: 0.75, 0.689              \n",
            "epoch: 1/1, 28/5432, 22s 801ms/step, lr: 0.0001, loss: 0.75, 0.769              \n",
            "epoch: 1/1, 29/5432, 23s 800ms/step, lr: 0.0001, loss: 0.749, 0.725              \n",
            "epoch: 1/1, 30/5432, 24s 798ms/step, lr: 0.0001, loss: 0.751, 0.785              \n",
            "epoch: 1/1, 31/5432, 25s 797ms/step, lr: 0.0001, loss: 0.75, 0.736              \n",
            "epoch: 1/1, 32/5432, 26s 797ms/step, lr: 0.0001, loss: 0.749, 0.726              \n",
            "epoch: 1/1, 33/5432, 26s 797ms/step, lr: 0.0001, loss: 0.749, 0.733              \n",
            "epoch: 1/1, 34/5432, 27s 797ms/step, lr: 0.0001, loss: 0.749, 0.755              \n",
            "epoch: 1/1, 35/5432, 28s 798ms/step, lr: 0.0001, loss: 0.747, 0.689              \n",
            "epoch: 1/1, 36/5432, 29s 796ms/step, lr: 0.0001, loss: 0.748, 0.784              \n",
            "epoch: 1/1, 37/5432, 29s 797ms/step, lr: 0.0001, loss: 0.747, 0.691              \n",
            "epoch: 1/1, 38/5432, 30s 796ms/step, lr: 0.0001, loss: 0.748, 0.791              \n",
            "epoch: 1/1, 39/5432, 31s 795ms/step, lr: 0.0001, loss: 0.749, 0.786              \n",
            "epoch: 1/1, 40/5432, 32s 796ms/step, lr: 0.0001, loss: 0.747, 0.687              \n",
            "epoch: 1/1, 41/5432, 33s 795ms/step, lr: 0.0001, loss: 0.748, 0.783              \n",
            "epoch: 1/1, 42/5432, 33s 796ms/step, lr: 0.0001, loss: 0.748, 0.732              \n",
            "epoch: 1/1, 43/5432, 34s 796ms/step, lr: 0.0001, loss: 0.748, 0.734              \n",
            "epoch: 1/1, 44/5432, 35s 795ms/step, lr: 0.0001, loss: 0.748, 0.781              \n",
            "epoch: 1/1, 45/5432, 36s 795ms/step, lr: 0.0001, loss: 0.748, 0.733              \n",
            "epoch: 1/1, 46/5432, 37s 795ms/step, lr: 0.0001, loss: 0.749, 0.777              \n",
            "epoch: 1/1, 47/5432, 37s 794ms/step, lr: 0.0001, loss: 0.749, 0.78              \n",
            "epoch: 1/1, 48/5432, 38s 796ms/step, lr: 0.0001, loss: 0.747, 0.651              \n",
            "epoch: 1/1, 49/5432, 39s 796ms/step, lr: 0.0001, loss: 0.747, 0.737              \n",
            "epoch: 1/1, 50/5432, 40s 795ms/step, lr: 0.0001, loss: 0.748, 0.785              \n",
            "epoch: 1/1, 51/5432, 41s 794ms/step, lr: 0.0001, loss: 0.749, 0.781              \n",
            "epoch: 1/1, 52/5432, 41s 795ms/step, lr: 0.0001, loss: 0.748, 0.696              \n",
            "epoch: 1/1, 53/5432, 42s 794ms/step, lr: 0.0001, loss: 0.748, 0.79              \n",
            "epoch: 1/1, 54/5432, 43s 794ms/step, lr: 0.0001, loss: 0.748, 0.734              \n",
            "epoch: 1/1, 55/5432, 44s 795ms/step, lr: 0.0001, loss: 0.747, 0.69              \n",
            "epoch: 1/1, 56/5432, 44s 794ms/step, lr: 0.0001, loss: 0.747, 0.73              \n",
            "epoch: 1/1, 57/5432, 45s 794ms/step, lr: 0.0001, loss: 0.747, 0.74              \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-6fc21378e7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#M1 = AutoModelForQuestionAnswering.from_pretrained(model_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-30dc3a7145bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_start_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}