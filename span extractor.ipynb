{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riccardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Train, Validation and Test splits\n",
    "\n",
    "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
    "\n",
    "We'll consider the provided validation set as a test set. <br>\n",
    "$\\rightarrow$ Write your own script to:\n",
    "* Split the train data in train and validation splits (80% train and 20% val)\n",
    "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
    "* Perform splitting using the following seed for reproducibility: 42\n",
    "\n",
    "#### Reproducibility Memo\n",
    "\n",
    "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('coqa', 'train.json'), 'r') as j:\n",
    "    train = json.loads(j.read())\n",
    "\n",
    "with open(os.path.join('coqa', 'test.json'), 'r') as j:\n",
    "    test = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train['data']\n",
    "test = test['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[len(doc['questions']) for doc in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=np.cumsum(np.array(lengths,dtype=np.float32))\n",
    "train_end=np.where((le/le[-1])>0.8)[0][0]\n",
    "\n",
    "validation = train[train_end : ] \n",
    "train = train[ : train_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5771\n",
      "1428\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86909 0.7999208445700295\n",
      "21738 0.20007915542997046\n"
     ]
    }
   ],
   "source": [
    "len_train=np.sum([len(doc['questions']) for doc in train])\n",
    "len_val=np.sum([len(doc['questions']) for doc in validation])\n",
    "\n",
    "len_tot=len_train+len_val\n",
    "print(len_train,len_train/len_tot)\n",
    "print(len_val,len_val/len_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, return_history=False):\n",
    "\n",
    "        self.story=[d['story'] for d in data]\n",
    "        self.questions=[d['questions'] for d in data]\n",
    "        self.answers=[d['answers'] for d in data]\n",
    "        lengths = [len(doc['questions']) for doc in data]\n",
    "        self.lengths = np.cumsum(np.array(lengths,dtype=np.int32))\n",
    "        self.R_H=return_history\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lengths[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f_idx=int(np.where(self.lengths > idx)[0][0])\n",
    "        if f_idx>0:\n",
    "            q_idx=idx-self.lengths[f_idx-1]\n",
    "        else:\n",
    "            q_idx=idx\n",
    "\n",
    "        passage=self.story[f_idx]\n",
    "        questions=self.questions[f_idx]\n",
    "        answers=self.answers[f_idx]\n",
    "        question=questions[q_idx]['input_text']\n",
    "        span_start=answers[q_idx]['span_start']\n",
    "        span_end=answers[q_idx]['span_end']\n",
    "        span_text=answers[q_idx]['span_text']\n",
    "\n",
    "        if self.R_H:\n",
    "            history = np.concatenate([ [questions[i]['input_text'], answers[i]['input_text']] for i in range(q_idx)],0)\n",
    "            return (passage,question,history), span_text\n",
    "\n",
    "        return (passage,question), (span_start, span_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# target is \"nice puppet\"\n",
    "target_start_index = torch.tensor([14])\n",
    "target_end_index = torch.tensor([15])\n",
    "\n",
    "\n",
    "\n",
    "outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "loss = outputs.loss\n",
    "round(loss.item(), 2)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(CustomImageDataset(train), batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(CustomImageDataset(validation), batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(CustomImageDataset(test), batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a21de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Task 3] Model definition\n",
    "\n",
    "Write your own script to define the following transformer-based models from [huggingface](https://HuggingFace.co/).\n",
    "\n",
    "* [M1] DistilRoBERTa (distilberta-base)\n",
    "* [M2] BERTTiny (bert-tiny)\n",
    "\n",
    "**Note**: Remember to install the ```transformers``` python package!\n",
    "\n",
    "**Note**: We consider small transformer models for computational reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_name = 'distilroberta-base'\n",
    "\n",
    "M1 = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "T1 = AutoTokenizer.from_pretrained(model_name, max_new_tokens=50)\n",
    "\n",
    "model_name = 'prajjwal1/bert-tiny'\n",
    "\n",
    "M2 = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "T2 = AutoTokenizer.from_pretrained(model_name, max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer):\n",
    "    model.to('cuda')\n",
    "\n",
    "    L=[]\n",
    "\n",
    "    model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (passage, question), (sep_start, sep_end) = data\n",
    "\n",
    "            #text_input = [question[i] + ' [SEP] ' + passage[i] for i in range(len(passage))]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                question,\n",
    "                #passage,\n",
    "                max_length=1_000,\n",
    "                #truncation=\"only_second\",\n",
    "                #stride=50,\n",
    "                #return_overflowing_tokens=True,\n",
    "                #return_offsets_mapping=True,\n",
    "                #padding=\"max_length\"\n",
    "            )\n",
    "            \n",
    "            print(inputs.sequence_ids(i))\n",
    "            print(np.sum(inputs.sequence_ids(i) is not None))\n",
    "\n",
    "            #get_target_position(inputs, sep_start, sep_end)\n",
    "\n",
    "            x = torch.tensor(inputs['input_ids']).to('cuda')\n",
    "            target_start_index = sep_start[inputs['overflow_to_sample_mapping']].to('cuda')\n",
    "            target_end_index = sep_end[inputs['overflow_to_sample_mapping']].to('cuda')\n",
    "\n",
    "            outputs = model(x, start_positions=target_start_index, end_positions=target_end_index)\n",
    "            loss = outputs.loss\n",
    "            #round(loss.item(), 2)\n",
    "\n",
    "            # the forward function automatically creates the correct decoder_input_ids\n",
    "            #loss = model(input_ids=X, labels=y).loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            L.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3g}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 10 at dim 1 (got 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(M1,T1)\n",
      "Cell \u001b[1;32mIn [44], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39msum(inputs\u001b[39m.\u001b[39msequence_ids(i) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m     36\u001b[0m \u001b[39m#get_target_position(inputs, sep_start, sep_end)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m target_start_index \u001b[39m=\u001b[39m sep_start[inputs[\u001b[39m'\u001b[39m\u001b[39moverflow_to_sample_mapping\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m target_end_index \u001b[39m=\u001b[39m sep_end[inputs[\u001b[39m'\u001b[39m\u001b[39moverflow_to_sample_mapping\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 10 at dim 1 (got 14)"
     ]
    }
   ],
   "source": [
    "train(M1,T1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bb5e395f71e970ef2dfa88b10e29155c2f154fbffe5a547ccb4cc942724aa68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
