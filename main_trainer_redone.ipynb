{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Assignment 2 - Question Answering with Transformers on CoQA</h1>\n",
    "    <h2>Natural Language Processing</h2>\n",
    "    <h3>Antonio Politano, Enrico Pittini, Riccardo Spolaor and Samuele Bortolato</h3>\n",
    "    <h4>antonio.politano2@studio.unibo.it, enrico.pittini@studio.unibo.it, riccardo.spolaor@studio.unibo.it, samuele.bortolato@studio.unibo.it</h4>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Assignment description: see `Assignment.ipynb`.\n",
    "\n",
    "In this notebook the QA task is addressed.\n",
    "\n",
    "For more detailed informations about the used functions, look into the corresponding docstrings inside the python files, inside the `utils` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for autoreloading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for reproducibility\n",
    "from utils.seeder import set_random_seed\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 1] Remove unaswerable QA pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset download\n",
    "\n",
    "The dataset is downloaded and saved in the `coqua` folder using the snippet of code provided in `Assignment.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataframe Creation\n",
    "\n",
    "The train and test dataframes (`train_df` and `test_df`) are built. Each row contains information about a specific question and the corresponding answer along with their chronological collocation (`turn_id`) in the conversation. Furthermore informations about the passage containing the context and the history of previous questions and answers of the relative conversation is contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataframe_builder import get_dataframe\n",
    "\n",
    "train_df = get_dataframe(os.path.join('coqa', 'train.json'))\n",
    "test_df = get_dataframe(os.path.join('coqa', 'test.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heads of the train and test dataframes are shown below along with their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train dataframe shape: {train_df.shape}')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test dataframe shape: {test_df.shape}')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the training dataframe contains $15$ different features, while the test dataframe has just $14$. In particular the train dataframe includes the additional features `question_bad_turn` and `answer_bad_turn`. With a quick inspection of the dataframe's head it can be observed that they include `NaN` values. Since the task requires to remove solely unanswerable question-answer pairs, not mentioning the handling of \"bad turn\", these two features are dropped.\n",
    "\n",
    "On the other hand the test dataframe contains the extra feature `additional_answers`, which can be removed as expressed in the specifications of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-matching features\n",
    "\n",
    "train_df.drop(['question_bad_turn', 'answer_bad_turn'], axis=1, inplace=True)\n",
    "test_df.drop('additional_answers', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition the features `name` and `filename` are removed since they are considered useless for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns (`name`, `filename`)\n",
    "\n",
    "train_df.drop(['name', 'filename'], axis=1, inplace=True)\n",
    "test_df.drop(['name', 'filename'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, by inspecting the `question_turn_id` and `answer_turn_id` it can be noticed that they are equivalent, since they refer to the same question-answer pair, hence they can be merged in a single feature (`turn_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the turn ids of the questions are the same as the respective answers\n",
    "\n",
    "assert train_df['question_turn_id'].equals(train_df['answer_turn_id']), \\\n",
    "    'Question and answer turn ids are different in the train dataset'\n",
    "    \n",
    "assert test_df['question_turn_id'].equals(test_df['answer_turn_id']), \\\n",
    "    'Question and answer turn ids are different in the test dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns `question_turn_id` and `answer_turn_id` into a singular `turn_id` column since they are equal\n",
    "refactor_turn_id_columns = lambda df: \\\n",
    "    df.drop('question_turn_id', axis=1).rename(columns = {'answer_turn_id': 'turn_id'})\n",
    "    \n",
    "train_df = refactor_turn_id_columns(train_df)\n",
    "test_df = refactor_turn_id_columns(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the columns `answer_input_text` and `question_input_text` are renamed into `answer` and `question` respectively for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns `answer_input_text` and `question_input_text` into `answer` and `question` respectively\n",
    "column_renames = {'answer_input_text': 'answer', 'question_input_text': 'question'}\n",
    "\n",
    "train_df.rename(columns=column_renames, inplace=True)\n",
    "test_df.rename(columns=column_renames, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the dataframes now match on the column number and no Null values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train dataframe shape after the unwanted columns drop: {train_df.shape}')\n",
    "print(f'Test dataframe shape after the unwanted column drop: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Null values in the train dataframe: {train_df.isna().sum().sum()}.')\n",
    "print(f'Null values in the test dataframe: {test_df.isna().sum().sum()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove Unanswerable Question-Answer Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As required by the task, the unanswerable question-answer pairs are removed from the dataset by dropping the rows of the dataframes where the feature `answer` is equal to \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows with unknown answer\n",
    "\n",
    "train_df.drop(train_df[train_df['answer'] == 'unknown'].index, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df.drop(test_df[test_df['answer'] == 'unknown'].index, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train dataframe shape after the unanswerable question-answer pairs are removed: {train_df.shape}')\n",
    "print(f'Test dataframe shape after the unanswerable question-answer pairs are removed: {test_df.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell it is asserted that the history was properly created for each Question-Answer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_history(df: pd.DataFrame, dataframe_name: str = None):\n",
    "    \"\"\"Check that the history is properly built for each Question-Answer pair in each row of the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The dataframe on which the history is checked.\n",
    "    dataframe_name : str, optional\n",
    "        The name of the dataframe. Defaults to None.\n",
    "    \"\"\"\n",
    "    prev_doc = None\n",
    "    prev_hist = []\n",
    "    prev_question = None\n",
    "    prev_answer = None\n",
    "    for d, h, q, a in zip(df['id'], df['history'], df['question'], df['answer']):\n",
    "        if d != prev_doc:\n",
    "            assert len(h) == 0, 'Error: Initial history of a new conversation is not empty!'\n",
    "            prev_doc = d\n",
    "            prev_hist = []\n",
    "            prev_question = q\n",
    "            prev_answer = a\n",
    "        else:\n",
    "            assert prev_hist + [prev_question, prev_answer] == h, 'Error: The history was not computed properly!'\n",
    "            prev_question = q\n",
    "            prev_answer = a\n",
    "            prev_hist = h\n",
    "\n",
    "    print(f'The history{f\" of {dataframe_name} dataframe\" if dataframe_name is not None else \"\"}', \n",
    "          'was properly built for each Question-Answer pair.')\n",
    "\n",
    "check_history(train_df, 'train')\n",
    "check_history(test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section some interesting analyses on the training set are carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataframe by `id`\n",
    "grouped_train_df = train_df.groupby(by=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of train passages: {len(grouped_train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_analisys import *\n",
    "plot_converstion_length_distribution(grouped_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_passage_length_analysis(grouped_train_df.story.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_answer_span_text_percentile(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Train, Validation and Test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the train dataframe is split into an actual train and a validation dataframes.\n",
    "\n",
    "The split is performed as follows:\n",
    "1. The random seed is set to $42$ for reproducibility purposes.\n",
    "2. The train proportion of the actual training dataset to the original dataset is of $0.8$.\n",
    "3. The train dataframe is shuffled and divided into the two new dataframes making sure that no conversation is split among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from typing import Tuple\n",
    "\n",
    "def train_validation_split(df: pd.DataFrame, train_size: int = .8, random_seed: int = 42) \\\n",
    "    -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" Get train and validation dataframes by shuffling and splitting an original dataframe according to a given proportion\n",
    "    and a specific random seed.\n",
    "    \n",
    "    Note: The order of the rows of the same conversation is preserved in the shuffle. Moreover, the conversations are never\n",
    "    split across the two resulting dataframes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The dataframe from which the train and validation dataframes are obtained.\n",
    "    train_size : int, optional\n",
    "        The proportion of the train split. Defaults to 0.8.\n",
    "    random_seed : int, optional\n",
    "        The random seed for the shuffle. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: _description_\n",
    "    \"\"\"\n",
    "    # Get indices of train and test rows in the dataframe\n",
    "    group_shuffle_split = GroupShuffleSplit(n_splits=2, train_size=train_size, random_state=random_seed)\n",
    "    train_ix, test_ix = next(group_shuffle_split.split(df, groups=df.id))\n",
    "\n",
    "    train_df = df.loc[train_ix]\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    val_df = df.loc[test_ix]\n",
    "    val_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_validation_split(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tail of the obtain dataframe (`train_df`) and the head of the validation dataframe (`val_df`) are shown below to assert that the conversations are not splitted and that their question-answer pairs are still chronologically ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train dataframe shape after the split: {train_df.shape}')\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation dataframe shape after the split: {val_df.shape}')\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train passages count: {len(train_df.groupby(by=[\"id\"]))}')\n",
    "print(f'Validation passages count: {len(val_df.groupby(by=[\"id\"]))}')\n",
    "\n",
    "print()\n",
    "\n",
    "len_tot=len(train_df)+len(val_df)\n",
    "print(f'Train QaA count: {len(train_df)} \\t\\t Train QaA ratio: {len(train_df)/len_tot:.2f}')\n",
    "print(f'Validation QaA count: {len(val_df)} \\t Validation QaA ratio: {len(val_df)/len_tot:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the train, validation and test dataloaders are provided for future training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader_builder import get_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_df, batch_size=8)\n",
    "val_dataloader = get_dataloader(val_df)\n",
    "test_dataloader = get_dataloader(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a21de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Task 3] Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'distilroberta-base'          # distil-roberta pretrained model\n",
    "model_name = 'prajjwal1/bert-tiny'          # tiny-bert pretrained model\n",
    "\n",
    "use_history=False\n",
    "seed = 1337\n",
    "\n",
    "folder='weigths'\n",
    "if use_history:\n",
    "    folder_name = f'{folder}\\PQH\\seed{seed}'\n",
    "else:\n",
    "    folder_name = f'{folder}\\PQ\\seed{seed}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Model \n",
    "\n",
    "set_random_seed(seed)\n",
    "model = Model(model_name=model_name, device='cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] Question generation with text passage $P$ and question $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "question_sample = [train_df.iloc[i]['question']]\n",
    "passage_sample = [train_df.iloc[i]['story']]\n",
    "answer_sample = train_df.iloc[i]['answer']\n",
    "\n",
    "print(f'Question sample: \"{question_sample[0]}\"')\n",
    "print()\n",
    "print(f'Predicted answer by the model: \"{model.generate(passage_sample, question_sample)[0]}\"')\n",
    "print()\n",
    "print(f'True answer: \"{answer_sample}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 5] Question generation with text passage $P$, question $Q$ and dialogue history $H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sample = [train_df.iloc[5]['question']]\n",
    "passage_sample = [train_df.iloc[5]['story']]\n",
    "answer_sample = train_df.iloc[5]['answer']\n",
    "history_sample = [' <sep> '.join(train_df.iloc[5]['history'])]\n",
    "\n",
    "print(f'Question sample: \"{question_sample[0]}\"')\n",
    "print()\n",
    "print(f'Predicted answer by the model: \"{model.generate(passage_sample, question_sample, history=history_sample)[0]}\"')\n",
    "print()\n",
    "print(f'True answer: \"{answer_sample}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 6] Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    checkpoint = torch.load(f\"{folder_name}\\\\{model_name.replace('/','_')}.pth\")\n",
    "    loss_history = checkpoint['loss_history']\n",
    "    val_loss_history = checkpoint['val_loss_history']\n",
    "    opt_state_dict=checkpoint['opt_state_dict']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Loaded saved files')\n",
    "except:\n",
    "    loss_history=None\n",
    "    val_loss_history=None\n",
    "    opt_state_dict=None\n",
    "    print('Unable to load saved files, default initialization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import train_1\n",
    "set_random_seed(seed)\n",
    "train_1( train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=3,\n",
    "        model=model, use_history=False, folder_name=folder_name,\n",
    "        #opt_state_dict = opt_state_dict, \n",
    "        loss_history = list(loss_history) if loss_history is not None else None,\n",
    "        val_loss_history = list(val_loss_history) if loss_history is not None else None,\n",
    "        steps_validate=0.33, steps_save=0.01, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"{folder_name}\\\\{model_name.replace('/','_')}.pth\")\n",
    "lh = checkpoint['loss_history']\n",
    "vlh = checkpoint['val_loss_history']\n",
    "\n",
    "N=100\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(lh[:,0])\n",
    "plt.plot(np.convolve(lh[:,0], np.ones(N)/N, mode='valid'))\n",
    "if len(vlh)>0:\n",
    "    plt.plot(vlh[:,0],vlh[:,1],'r*')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(lh[:,1])\n",
    "plt.plot(np.convolve(lh[:,1], np.ones(N)/N, mode='valid'))\n",
    "if len(vlh)>0:\n",
    "    plt.plot(vlh[:,0],vlh[:,2],'r*')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(lh[:,0])\n",
    "plt.plot(np.convolve(lh[:,0], np.ones(N)/N, mode='valid'))\n",
    "if len(vlh)>0:\n",
    "    plt.plot(vlh[:,0],vlh[:,1],'r*')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(lh[:,1])\n",
    "plt.plot(np.convolve(lh[:,1], np.ones(N)/N, mode='valid'))\n",
    "if len(vlh)>0:\n",
    "    plt.plot(vlh[:,0],vlh[:,2],'r*')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.squad import validate\n",
    "f1_squad = validate(model, val_dataloader, use_history=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 7] Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'distilroberta-base'          # distil-roberta pretrained model\n",
    "model_name = 'prajjwal1/bert-tiny'          # tiny-bert pretrained model\n",
    "\n",
    "use_history=False\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_history:\n",
    "    folder_name = 'weigths\\PQH\\seed'+str(seed)\n",
    "else:\n",
    "    folder_name = 'weigths\\PQ\\seed'+str(seed)\n",
    "\n",
    "set_random_seed(seed)\n",
    "model = Model(model_name=model_name, device='cuda')\n",
    "\n",
    "model.load_state_dict(torch.load(f\"{folder_name}/{model_name.replace('/','_')}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.squad import _compute_squad_f1\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_worst_answers(model: Model, df_source, use_history: bool = False, k=5, min_answer_length=1):\n",
    "    # (f1, question, passage, history if, gold_answer, pred_answer)\n",
    "    worst_answers = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    source_dataloader = get_dataloader(df=df_source, batch_size=16)\n",
    "\n",
    "    for batch_idx, data in tqdm(enumerate(source_dataloader, 0)):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (passage, question, history), (answer, _, _) = data\n",
    "            \n",
    "            pred = model.generate(passage,question,history if use_history else None)\n",
    "            \n",
    "            if min_answer_length > 1:\n",
    "                mask = np.array([len(predicted.split(' ')) >= min_answer_length for predicted in pred])\n",
    "                passage = np.array(passage)[mask]\n",
    "                question = np.array(question)[mask]\n",
    "                history = np.array(history)[mask]\n",
    "                answer = np.array(answer)[mask]\n",
    "                pred = np.array(pred)[mask]\n",
    "\n",
    "            f1_scores = np.array([_compute_squad_f1(gold,predicet) for gold, predicet in zip(answer,pred)])\n",
    "            samples_indices = np.argsort(f1_scores)[:k]\n",
    "\n",
    "            worst_answers += [(f1_scores[sample_idx], question[sample_idx], passage[sample_idx], history[sample_idx], \n",
    "                               answer[sample_idx], pred[sample_idx]) \n",
    "                              for sample_idx in samples_indices]\n",
    "            worst_answers = sorted(worst_answers)[:k]\n",
    "    \n",
    "    return worst_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_df.groupby(by=['source']))\n",
    "next(it)\n",
    "#next(it)\n",
    "df_source = next(it)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataloader = get_dataloader(df=df_source, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(source_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_worst_answers(model, df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_df.groupby(by=['source']))\n",
    "next(it)\n",
    "df_source = next(it)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = get_worst_answers(M2, df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_token_importances(model, question, passage, span_start, span_end, history=None):\n",
    "\n",
    "    token_importances = model.compute_token_importances(passage, question, span_start, span_end, history)\n",
    "\n",
    "    y = np.zeros(shape=(token_importances.shape[1],))\n",
    "\n",
    "    y[span_start : span_end] = 1\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(token_importances.cpu().detach()[0])\n",
    "    plt.plot(y)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_df.iloc[0]\n",
    "show_token_importances(model, r['question'], r['story'], r['answer_span_start'],  r['answer_span_end'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "506daaeaa79bc5d30715519e2bba71fd2fb898b1f12d903345e89400e3b4f753"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
